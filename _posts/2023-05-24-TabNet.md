---
layout: single
title: "(논문정리) TabNet : Attentive Interpretable Tabular Learning"
categories: machine_learning
tags: [attention, tabular_data, decision_tree]
use_math: true
publish: false
author_profile: false
---

ICLR 2020 논문 [TabNet : Attentive Interpretable Tabular Learning](https://ojs.aaai.org/index.php/AAAI/article/view/16826)에 대해 아주 조금 공부한 것을 여기 적습니다.
3장 1절까지만, 그것도 대략적으로만 정리하려고 합니다.
게다가 2장은 related work이니 쓰지 않으려고 합니다.
기록하려고 했던 건 3장 1절의 그림 (figure 2)에 대한 계산이었습니다.

3장 1절 이후의 내용은 적지 않았지만, 목차는 간단히 다 적어놓겠습니다.

**abstract**

(오역이 있을 수 있음) 이 논문에은 해석가능한 고성능의 deep tabular data learning network인 TabNet을 소개합니다.
TabNet은 sequential attention mechanism을 사용하는데, 이 mechanism은 각 decision step마다 feature들을 선택하고 이것들을 합쳐서 final prediction decision에 사용합니다.
feature를 선택할 때에는 sparse하게 feature를 선택하게 되므로 학습은 굉장히 효율적으로 진행되며, 성능도 좋습니다.
이러한 희소성(sparsity)는 의사결정(decision making)을 해석가능하게(interpretable) 만들어줍니다.
TabNet은 다른 신경망이나 decision tree 기반 아키텍쳐보다 좋은 성능을 보이고 또한 attention과 sparse feature selection을 사용했기 때문에 모델의 결과를 해석할 수 있다는 장점이 있습니다.

# 1. Introduction

(1)
deep learning for tabular data remains under-explored, with variants of ensemble decision trees(DTs) still dominating most applications.
Why?

First,
(i) DTs have representation power for decision manifolds with approximately hyperplane boundaries which are common in tabular data.
(ii) DTs are easy to develop and fast to train.
(iii) DTs are highly interpretable in their basic form(e.g. by tracking decision nodes and edges).

Second,
previously-proposed DNN architectures (stacked CNN or MLP) may not be the best fit for tabular data decision manifolds due to being vastly overparametrized.

(２)
Why is deep learning worth exploring for tabular data?
One obvious motivation : pushing the performance albeit an increased computational cost, especially with more training data.
Additional benefits : gradient-based end-to-end learning

(３) TabNet summarization :

1. end-to-end : no preprocessing, flexible integration
2. sequential attention : interpretability and reasoning
3. two valuable properties : (i) outperforming performance on both classification and regression from diffrent domains, (ii) interpretability
4. significant performance improvements by using unsupervised pre-training to predict masked features.



# 2 Related Work
 - Feature selection
 - Tree-based learning
 - Integration of DNNs into DTs
 - Self-supservised learning



# 3 TabNet for Tabular Learning
## 3.1 Principles

![]({{site.url}}\images\2023-05-24-TabNet\fig_2.png){: .img-100-center}

$$
\begin{align*}
f(x_1,x_2)
&=
\text{softmax}\left(
    \text{ReLU}\left(
        x_1W_1 + b_1
    \right)+
    \text{ReLU}\left(
        x_2W_1 + b_2
    \right)
    \right)\\[10pt]
&=
\text{softmax}\left(
    \text{ReLU}\left(
        x_1\begin{bmatrix}c_1\\-c_1\\0\\0\end{bmatrix} + \begin{bmatrix}-ac_1\\ac_1\\-1\\-1\end{bmatrix}
    \right)+
    \text{ReLU}\left(
        x_1\begin{bmatrix}0\\0\\c_2\\-c_2\end{bmatrix} + \begin{bmatrix}-1\\1\\-dc_2\\dc_2\end{bmatrix}
    \right)
    \right)\\[10pt]
&=
\text{softmax}\left(
    \text{ReLU}\left(
        \begin{bmatrix}c_1(x_1-a)\\-c_1(x_1-a)\\-1\\-1\end{bmatrix}
    \right)+
    \text{ReLU}\left(
        \begin{bmatrix}-1\\-1\\c_2(x_1-d)\\-c_2(x_1-d)\end{bmatrix}
    \right)
    \right)\\[10pt]
&=
\text{softmax}\left(
    \begin{bmatrix}\text{ReLU}\left(c_1(x_1-a)\right)\\\text{ReLU}\left(-c_1(x_1-a)\right)\\0\\0\end{bmatrix}
    +
    \begin{bmatrix}0\\0\\\text{ReLU}\left(c_2(x_2-d)\right)\\\text{ReLU}\left(-c_2(x_2-d)\right)\end{bmatrix}
    \right)\\[10pt]
&=
\text{softmax}\left(
    \begin{bmatrix}\text{ReLU}\left(c_1(x_1-a)\right)\\\text{ReLU}\left(-c_1(x_1-a)\right)\\\text{ReLU}\left(c_2(x_2-d)\right)\\\text{ReLU}\left(-c_2(x_2-d)\right)\end{bmatrix}
    \right)\\[10pt]
&=\begin{cases}
\frac1{\exp\left(c_1(x_1-a)\right)+\exp\left(c_2(x_2-d)\right)}
\begin{bmatrix}
\exp\left(c_1(x_1-a)\right)\\
0\\
\exp\left(c_2(x_2-d)\right)\\
0
\end{bmatrix}
&(x_1>a,\quad x_2<d)\\[15pt]
\frac1{\exp\left(c_1(x_1-a)\right)+\exp\left(-c_2(x_2-d)\right)}
\begin{bmatrix}
\exp\left(c_1(x_1-a)\right)\\
0\\
0\\
\exp\left(-c_2(x_2-d)\right)
\end{bmatrix}
&(x_1<a,\quad x_2>d)\\[15pt]
\frac1{\exp\left(-c_1(x_1-a)\right)+\exp\left(c_2(x_2-d)\right)}
\begin{bmatrix}
0\\
\exp\left(-c_1(x_1-a)\right)\\
\exp\left(c_2(x_2-d)\right)\\
0
\end{bmatrix}
&(x_1<a,\quad x_2<d)\\[15pt]
\frac1{\exp\left(-c_1(x_1-a)\right)+\exp\left(-c_2(x_2-d)\right)}
\begin{bmatrix}
0\\
\exp\left(-c_1(x_1-a)\right)\\
0\\
\exp\left(-c_2(x_2-d)\right)
\end{bmatrix}
&(x_1<a,\quad x_2<d)\\[15pt]
\end{cases}
\end{align*}
$$

## 3.2 Overall Architecture

# 4 Experiments

## 4.1 Performance

## 4.2 Interpretability

# 5 Conclusions