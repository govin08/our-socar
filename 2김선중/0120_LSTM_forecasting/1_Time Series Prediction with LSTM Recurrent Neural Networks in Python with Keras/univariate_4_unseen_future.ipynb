{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter들을 바꿔가면서 괜찮은 모델을 만들어낸다.\n",
    "- 기본 모델\n",
    "    - one layer LSTM (+stateful)\n",
    "    - batch size = 1\n",
    "    - minmax 후 학습 -> 이후 다시 inverse_transform\n",
    "    - (중요) unseen data에 대한 고려\n",
    "    - train-test split - 7:3\n",
    "- evaluation metric\n",
    "    - MSE\n",
    "- hyperparameters\n",
    "    - `units` = 4\n",
    "    - `epochs` = 100\n",
    "    - `look_back` = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with memory\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 44)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 - 4s - loss: 0.0055 - 4s/epoch - 39ms/step\n",
      "96/96 - 0s - loss: 0.0097 - 274ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0075 - 253ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0061 - 256ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0054 - 270ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0050 - 265ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0048 - 250ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0047 - 259ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0046 - 262ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0045 - 275ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0044 - 300ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0044 - 285ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0043 - 270ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0042 - 296ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0042 - 269ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0041 - 252ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0041 - 283ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0041 - 277ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0040 - 272ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0040 - 257ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0040 - 272ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0039 - 272ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0039 - 277ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0039 - 290ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0038 - 275ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0038 - 255ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0038 - 275ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0038 - 319ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0037 - 306ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0037 - 277ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0037 - 250ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0037 - 279ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0037 - 274ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0036 - 267ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0036 - 253ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0036 - 282ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0036 - 279ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0036 - 260ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0036 - 256ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0035 - 278ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0035 - 303ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0035 - 269ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0035 - 278ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0035 - 260ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0035 - 242ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0034 - 274ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0034 - 267ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0034 - 275ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0034 - 265ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0034 - 259ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0034 - 309ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0033 - 242ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0033 - 296ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0033 - 282ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0033 - 271ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0033 - 283ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0032 - 287ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0032 - 259ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0032 - 331ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0032 - 283ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0032 - 249ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0032 - 283ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0031 - 275ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0031 - 268ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0031 - 254ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0031 - 281ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0031 - 266ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0030 - 256ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0030 - 269ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0030 - 282ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0030 - 284ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0030 - 300ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0029 - 281ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0029 - 291ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0029 - 277ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0029 - 287ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0029 - 287ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0029 - 272ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0028 - 279ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0028 - 284ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0028 - 262ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0028 - 283ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0028 - 279ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0028 - 282ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0027 - 294ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0027 - 286ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0027 - 274ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0027 - 279ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0027 - 283ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0027 - 280ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0026 - 278ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0026 - 308ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0026 - 297ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0026 - 291ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0026 - 317ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0026 - 317ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0025 - 271ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0025 - 280ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0025 - 270ms/epoch - 3ms/step\n",
      "96/96 - 0s - loss: 0.0025 - 281ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(testX[0], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (1, 3, 1) for input KerasTensor(type_spec=TensorSpec(shape=(1, 3, 1), dtype=tf.float32, name='lstm_3_input'), name='lstm_3_input', description=\"created by layer 'lstm_3_input'\"), but it was called on an input with incompatible shape (1,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"lstm_3\" is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: (1,)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(1,), dtype=int32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# trainX.shape, testX.shape\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49mexpand_dims(testX[\u001b[39m0\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mshape, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5_5df8bv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\govin\\anaconda3\\envs\\aiffelthon\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"lstm_3\" is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: (1,)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(1,), dtype=int32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# trainX.shape, testX.shape\n",
    "model.predict(np.expand_dims(testX[0], axis=0).shape, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7a60c8f2c8d29e3138bb6141818b2f9370cc55944ffe85f7c0c63df44a79ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
